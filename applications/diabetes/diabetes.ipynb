{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset using Pandas\n",
    "data = pd.read_csv('diabetes.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "# For x: Extract out the dataset for input (all cols except the last)\n",
    "# For y: Extract out the dataset for output (last col)\n",
    "# Convert x & y to numpy usind .values\n",
    "\n",
    "x = data.iloc[:, 0:-1].values   # 'ilock' > use array indexing in np\n",
    "y_string = list(data.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the y data into values\n",
    "y_int = []\n",
    "for element in y_string:\n",
    "    if element == \"positive\":\n",
    "        y_int.append(1)\n",
    "    else:\n",
    "        y_int.append(0)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert list to np array\n",
    "y = np.array(y_int, dtype='float64')    # float bez x is of float type"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Normalisation in range (-1,1) using StandardScaler\n",
    "sc = StandardScaler()\n",
    "x = sc.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Arrays to Tensors\n",
    "x = torch.tensor(x)\n",
    "y = torch.tensor(y).unsqueeze(1)\n",
    "# unsquesed to add a dimension"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating and Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create class dataset\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "# self is used to refrence to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data to database for batch processing and shifting\n",
    "train_loader = DataLoader(dataset = dataset,\n",
    "           batch_size = 32,\n",
    "           shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):         #inherit from nn.Module\n",
    "    # Initalisation function\n",
    "    def __init__(self, input_feature, output_features):\n",
    "        super(Model, self).__init__()\n",
    "        # define the layers, ie the attributes\n",
    "        self.fc1 = nn.Linear(input_feature, 5)        # fc > fully connected layer, MLB or linear layer\n",
    "        self.fc2 = nn.Linear(5, 4)        \n",
    "        self.fc3 = nn.Linear(4, 3)        \n",
    "        self.fc4 = nn.Linear(3, output_features) \n",
    "        # define the activation functions, ie the functionallites  \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # Every function needs to have attributes (layers of NN) and functionalities (froward & back propogation function)\n",
    "\n",
    "    # Froward Propogation Function\n",
    "    def forward(self, x):\n",
    "        # passing the data through NN\n",
    "        out = self.fc1(x)       # pass throug layer\n",
    "        out = self.tanh(out)    #pass through activation function\n",
    "        out = self.fc2(out)\n",
    "        out = self.tanh(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.tanh(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "# Defining back proppogation method is not required as pytorch would do it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network (an object of the Net class)\n",
    "net = Model(7, 1)\n",
    "# No. of input feature/layer is 7 &  No. of output feature/layer is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss function ie BCE (BCE bez the output needs to be 0 or 1)\n",
    "criterion = nn.BCELoss(size_average= True)      #size_average=True > Losses are averaged over observations for each minibatch & get one value for loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Optimisation function ie SGD with momentum with learning rate of 0.1\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr = 0.1, momentum = 0.9)       # parameters is the attributes/ weights for each layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyper-parameter ie no. of epochs\n",
    "epochs = 200\n",
    "\n",
    "for epoch in range(200):\n",
    "    for (inputs, labels) in train_loader:\n",
    "        inputs = inputs.float()\n",
    "        labels = labels.float()\n",
    "        # Forward prop.\n",
    "        outputs = net(inputs)\n",
    "        # Loss Calculation\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Begin forward propagation\n",
    "        # STEP1 clear gradient buffer (w = w - lr*gradient)\n",
    "        optimizer.zero_grad()\n",
    "        # STEP2 calculate the gradient // backpropagation\n",
    "        loss.backward()\n",
    "        # STEP3 update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calculate training accuracy\n",
    "    output = (outputs > 0.5).float()\n",
    "\n",
    "    # Calculate average accuracy    (output == labels).sum() / output.shape[0]\n",
    "    accuracy = (output == labels).float().mean()\n",
    "\n",
    "    # Print Statistics\n",
    "    print(\"Epoch {}/{}, Loss: {:.3f}, Accuracy: {:.3f}\".format(epoch+1, epochs, loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
